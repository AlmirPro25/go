# ðŸ¤– Guia de Modelos Gemini - Sistema Completo

## ðŸ“‹ Modelos DisponÃ­veis

### Gemini 2.0 (Experimental - Mais Recente)

```typescript
const GEMINI_2_MODELS = {
  flash: "gemini-2.0-flash-exp",           // RÃ¡pido, 2M tokens
  thinking: "gemini-2.0-flash-thinking-exp", // Com raciocÃ­nio
  pro: "gemini-2.0-pro-exp"                // Mais poderoso
};
```

**CaracterÃ­sticas:**
- âœ… 2 milhÃµes de tokens de contexto
- âœ… Multimodal (texto, imagem, Ã¡udio, vÃ­deo)
- âœ… Streaming nativo
- âœ… Grounding com Google Search
- âš ï¸ Experimental (pode ter instabilidade)

### Gemini 1.5 (EstÃ¡vel - Recomendado para ProduÃ§Ã£o)

```typescript
const GEMINI_STABLE_MODELS = {
  flash: "models/gemini-flash-latest",     // RÃ¡pido e estÃ¡vel
  flashExp: "gemini-1.5-flash-exp-0827",   // Experimental
  pro: "models/gemini-pro-latest",         // Mais poderoso
  proExp: "gemini-1.5-pro-exp-0827"        // Pro experimental
};
```

**CaracterÃ­sticas:**
- âœ… EstÃ¡vel e confiÃ¡vel
- âœ… 1 milhÃ£o de tokens de contexto
- âœ… Multimodal
- âœ… ProduÃ§Ã£o-ready
- âœ… Melhor custo-benefÃ­cio

### Embeddings

```typescript
const EMBEDDING_MODELS = {
  latest: "text-embedding-004",            // Recomendado
  legacy: "embedding-001"                  // Legado
};
```

## ðŸ”§ ConfiguraÃ§Ã£o no Sistema

### 1. VariÃ¡veis de Ambiente

```bash
# .env
VITE_GEMINI_API_KEY=sua_chave_aqui

# Usar Gemini 2.0 (experimental)
USE_GEMINI_2=true

# OU usar modelo estÃ¡vel (recomendado)
USE_GEMINI_2=false

# Modelo especÃ­fico (opcional)
GEMINI_MODEL=models/gemini-flash-latest
```

### 2. ConfiguraÃ§Ã£o por ServiÃ§o

```typescript
// services/GeminiService.ts
const getModelConfig = () => {
  const useGemini2 = process.env.USE_GEMINI_2 === 'true';
  
  return {
    primary: useGemini2 
      ? 'gemini-2.0-flash-exp' 
      : 'models/gemini-flash-latest',
    
    fallback: 'models/gemini-flash-latest',
    
    embeddings: 'text-embedding-004'
  };
};
```

### 3. Fallback AutomÃ¡tico

```typescript
async function generateWithFallback(prompt: string) {
  const config = getModelConfig();
  
  try {
    // Tentar modelo primÃ¡rio
    const model = genAI.getGenerativeModel({ 
      model: config.primary 
    });
    return await model.generateContent(prompt);
    
  } catch (error) {
    console.warn('Fallback para modelo estÃ¡vel:', error);
    
    // Fallback automÃ¡tico
    const fallbackModel = genAI.getGenerativeModel({ 
      model: config.fallback 
    });
    return await fallbackModel.generateContent(prompt);
  }
}
```

## ðŸŽ¯ RecomendaÃ§Ãµes por Caso de Uso

### Desenvolvimento e Testes
```typescript
{
  model: "models/gemini-flash-latest",
  reason: "EstÃ¡vel, rÃ¡pido, confiÃ¡vel"
}
```

### ProduÃ§Ã£o (Alta Performance)
```typescript
{
  model: "models/gemini-flash-latest",
  reason: "Melhor custo-benefÃ­cio, estÃ¡vel"
}
```

### ProduÃ§Ã£o (MÃ¡xima Qualidade)
```typescript
{
  model: "models/gemini-pro-latest",
  reason: "Respostas mais elaboradas"
}
```

### ExperimentaÃ§Ã£o (Recursos Novos)
```typescript
{
  model: "gemini-2.0-flash-exp",
  fallback: "models/gemini-flash-latest",
  reason: "Testar recursos Gemini 2.0"
}
```

### RAG e Embeddings
```typescript
{
  embeddings: "text-embedding-004",
  generation: "models/gemini-flash-latest",
  reason: "Melhor para busca semÃ¢ntica"
}
```

### Fintech (Compliance)
```typescript
{
  model: "models/gemini-pro-latest",
  temperature: 0.1,
  reason: "Respostas mais precisas e consistentes"
}
```

## ðŸ“Š ComparaÃ§Ã£o de Modelos

| Modelo | Contexto | Velocidade | Custo | Estabilidade | Uso |
|--------|----------|------------|-------|--------------|-----|
| gemini-2.0-flash-exp | 2M tokens | âš¡âš¡âš¡ | ðŸ’° | âš ï¸ Experimental | Testes |
| gemini-flash-latest | 1M tokens | âš¡âš¡âš¡ | ðŸ’° | âœ… EstÃ¡vel | ProduÃ§Ã£o |
| gemini-pro-latest | 1M tokens | âš¡âš¡ | ðŸ’°ðŸ’° | âœ… EstÃ¡vel | Qualidade |
| gemini-2.0-pro-exp | 2M tokens | âš¡ | ðŸ’°ðŸ’°ðŸ’° | âš ï¸ Experimental | Pesquisa |

## ðŸ”„ MigraÃ§Ã£o de Modelos

### De Gemini 1.5 para 2.0

```typescript
// ANTES (Gemini 1.5)
const model = genAI.getGenerativeModel({ 
  model: 'models/gemini-flash-latest' 
});

// DEPOIS (Gemini 2.0 com fallback)
const modelName = process.env.USE_GEMINI_2 === 'true'
  ? 'gemini-2.0-flash-exp'
  : 'models/gemini-flash-latest';

const model = genAI.getGenerativeModel({ model: modelName });
```

### Atualizar Neural Core

```typescript
// neural-core/src/index.ts
const MODELS = {
  primary: process.env.USE_GEMINI_2 === 'true'
    ? 'gemini-2.0-flash-exp'
    : 'models/gemini-flash-latest',
  
  embeddings: 'text-embedding-004'
};
```

### Atualizar Proxy Server

```typescript
// proxy-server/src/server.ts
app.post('/api/generate', async (c) => {
  const modelName = c.req.header('X-Use-Gemini-2') === 'true'
    ? 'gemini-2.0-flash-exp'
    : 'models/gemini-flash-latest';
  
  const model = genAI.getGenerativeModel({ model: modelName });
  // ...
});
```

## ðŸ§ª Testar Modelos

### Script de Teste

```typescript
// test-models.ts
import { GoogleGenerativeAI } from '@google/generative-ai';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

async function testModel(modelName: string) {
  console.log(`\nTestando: ${modelName}`);
  
  try {
    const model = genAI.getGenerativeModel({ model: modelName });
    const result = await model.generateContent('OlÃ¡, vocÃª estÃ¡ funcionando?');
    const response = result.response.text();
    
    console.log('âœ… Sucesso:', response.substring(0, 50) + '...');
    return true;
  } catch (error) {
    console.error('âŒ Erro:', error.message);
    return false;
  }
}

async function testAllModels() {
  const models = [
    'gemini-2.0-flash-exp',
    'models/gemini-flash-latest',
    'models/gemini-pro-latest',
    'text-embedding-004'
  ];
  
  for (const model of models) {
    await testModel(model);
  }
}

testAllModels();
```

### Executar Teste

```bash
# Instalar dependÃªncias
npm install @google/generative-ai

# Executar teste
npx tsx test-models.ts
```

## ðŸ“ ConfiguraÃ§Ã£o Recomendada

### Para Desenvolvimento

```bash
# .env.development
USE_GEMINI_2=false
GEMINI_MODEL=models/gemini-flash-latest
GEMINI_TEMPERATURE=0.7
```

### Para ProduÃ§Ã£o

```bash
# .env.production
USE_GEMINI_2=false
GEMINI_MODEL=models/gemini-flash-latest
GEMINI_TEMPERATURE=0.5
ENABLE_FALLBACK=true
```

### Para ExperimentaÃ§Ã£o

```bash
# .env.experimental
USE_GEMINI_2=true
GEMINI_MODEL=gemini-2.0-flash-exp
GEMINI_FALLBACK=models/gemini-flash-latest
GEMINI_TEMPERATURE=0.8
```

## ðŸš¨ Troubleshooting

### Erro: "Model not found"

```typescript
// SoluÃ§Ã£o: Usar nome completo do modelo
âŒ model: "gemini-flash-latest"
âœ… model: "models/gemini-flash-latest"
```

### Erro: "Quota exceeded"

```typescript
// SoluÃ§Ã£o: Implementar rate limiting
import { RateLimiter } from 'limiter';

const limiter = new RateLimiter({
  tokensPerInterval: 60,
  interval: 'minute'
});

await limiter.removeTokens(1);
const result = await model.generateContent(prompt);
```

### Erro: "Context length exceeded"

```typescript
// SoluÃ§Ã£o: Truncar contexto
function truncateContext(text: string, maxTokens: number = 30000) {
  const estimatedTokens = text.length / 4;
  if (estimatedTokens > maxTokens) {
    const maxChars = maxTokens * 4;
    return text.substring(0, maxChars);
  }
  return text;
}
```

## ðŸ“š Recursos

- [Gemini API Docs](https://ai.google.dev/docs)
- [Modelos DisponÃ­veis](https://ai.google.dev/models/gemini)
- [Pricing](https://ai.google.dev/pricing)
- [Rate Limits](https://ai.google.dev/docs/rate_limits)

---

**Atualizado em:** 19/11/2025
**VersÃ£o:** 2.0.0
**Status:** âœ… Pronto para uso
